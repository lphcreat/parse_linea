######
###### This config file is a etl config
######

env {
  # You can set spark configuration here
  spark.app.name = "ETL-create"
  spark.executor.instances = 4
  spark.executor.cores = 2
  spark.executor.memory = "2g"
  spark.sql.catalogImplementation = "hive"
  spark.hadoop.hive.exec.dynamic.partition = "true"
  spark.hadoop.hive.exec.dynamic.partition.mode = "nonstrict"
}

source {
  # 定义数据源
  hive {
    pre_sql = """SELECT l.apply_id AS apply_id, r.settle_amount AS settle_amount, 
    r.settle_date AS settle_date, r.loan_channel_name AS loan_channel_name, 
    coalesce(p.bank_account_name,r.bank_account_name) AS bank_account_name, 
    coalesce(p.bank_account,r.bank_account) AS bank_account, 
    coalesce(p.bank_name,r.bank_name) AS bank_name 
    FROM person_loan.loan_tb r LEFT JOIN person_loan.biz_loan_plan p ON r.plan_id = p.plan_id 
    LEFT JOIN person_loan.apply_tb l ON p.approval_id = l.approval_id 
    LEFT JOIN person_loan.order_tb o ON p.order_id = o.order_id 
    WHERE r.loan_status = 'success'"""
    result_table_name = "tb1"
  }

  hive {
    pre_sql = """SELECT key_id,apply_id FROM warehouse.dim_pl_key"""
    result_table_name = "tb2"
  }

}

transform {
  #left join datasets
  sql {
    sql = "select tb1.*,tb2.key_id from tb1 left join tb2 on tb1.apply_id=tb2.apply_id"
    result_table_name = "union_table"
  }

  # get ids
  GetIds {
    union_col = "apply_id"
    target_col = "temp_id"
    #key_id 默认值为uid
    source_table_name = "union_table"
    result_table_name = "ids_table"
  }

  # 写入维度表与事实表
  sql {
    sql = """INSERT OVERWRITE TABLE warehouse.fact_pl_loan  
    SELECT uid,coalesce(key_id,-1) key_id,bank_account,bank_account_name,
    bank_name,loan_channel_name,settle_amount,date_format(settle_date,'yyyy-MM-dd') date_id FROM ids_table"""
  }

}

sink {


}